{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9189a7-1746-4db8-9712-57f23b97737d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: customers\nProcessing: products\nProcessing: orders\nProcessing: orderitems\nProcessing: deliveryperformance\nProcessing: customerfeedback\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date , round , to_timestamp , to_utc_timestamp , when\n",
    "from pyspark.sql.types import IntegerType, FloatType , DoubleType\n",
    "\n",
    "\n",
    "\n",
    "def transform_customers(df):\n",
    "    return df.withColumn(\"registration_date\", to_date(col(\"registration_date\"), \"yyyy-MM-dd\")) \\\n",
    "             .withColumn(\"total_orders\", col(\"total_orders\").cast(IntegerType())) \\\n",
    "             .withColumn(\"avg_order_value\",col(\"avg_order_value\").cast(DoubleType())) \\\n",
    "             .withColumn(\"avg_order_value\", round(col(\"avg_order_value\"), 2)) \\\n",
    "             .dropDuplicates([\"customer_id\"])\n",
    "\n",
    "def transform_products(df):\n",
    "    return df.withColumn(\"price\", round(col(\"price\").cast(DoubleType()),2)) \\\n",
    "             .withColumn(\"mrp\", round(col(\"mrp\").cast(DoubleType()),2)) \\\n",
    "             .withColumn(\"margin_percentage\", round(col(\"margin_percentage\").cast(DoubleType()),2)) \\\n",
    "             .withColumn(\"shelf_life_days\", col(\"shelf_life_days\").cast(IntegerType())) \\\n",
    "             .withColumn(\"min_stock_level\", col(\"min_stock_level\").cast(IntegerType())) \\\n",
    "             .withColumn(\"max_stock_level\", col(\"max_stock_level\").cast(IntegerType())) \\\n",
    "             .dropDuplicates([\"product_id\"])\n",
    "\n",
    "def transform_orders(df):\n",
    "    return df.withColumn(\"order_date\", to_timestamp(col(\"order_date\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "             .withColumn(\"promised_delivery_time\", to_timestamp(col(\"promised_delivery_time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "             .withColumn(\"actual_delivery_time\", to_timestamp(col(\"actual_delivery_time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "             .withColumn(\"order_total\", round(col(\"order_total\").cast(DoubleType()),2)) \\\n",
    "             .dropDuplicates([\"order_id\"])\n",
    "\n",
    "def transform_orderitems(df):\n",
    "    return df.withColumn(\"quantity\", col(\"quantity\").cast(IntegerType())) \\\n",
    "             .withColumn(\"unit_price\", round(col(\"unit_price\").cast(DoubleType()),2)) \\\n",
    "             .dropDuplicates([\"order_id\"])\n",
    "\n",
    "def transform_deliveryperformance(df):\n",
    "    return df.withColumn(\"reasons_if_delayed\", when(col(\"reasons_if_delayed\").isNull(), \"No Delay\").otherwise(col(\"Reasons_if_Delayed\"))) \\\n",
    "             .withColumn(\"delivery_time_minutes\",col(\"delivery_time_minutes\").cast(DoubleType())) \\\n",
    "             .withColumn(\"distance_km\", round(col(\"distance_km\").cast(DoubleType()),2)) \\\n",
    "             .withColumn(\"promised_time\", to_timestamp(col(\"promised_time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "             .withColumn(\"actual_time\", to_timestamp(col(\"actual_time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "             .dropDuplicates([\"order_id\"])\n",
    "\n",
    "def transform_customerfeedback(df):\n",
    "    return df.withColumn(\"rating\", col(\"rating\").cast(IntegerType())) \\\n",
    "             .withColumn(\"feedback_date\", to_date(col(\"feedback_date\"), \"yyyy-MM-dd\")) \\\n",
    "             .dropDuplicates([\"order_id\"])\n",
    "\n",
    "\n",
    "\n",
    "# Define a dictionary of transformations for each table\n",
    "transformations = {\n",
    "    \"customers\": transform_customers,\n",
    "    \"products\": transform_products,\n",
    "    \"orders\": transform_orders,\n",
    "    \"orderitems\": transform_orderitems,\n",
    "    \"deliveryperformance\": transform_deliveryperformance,\n",
    "    \"customerfeedback\": transform_customerfeedback,\n",
    "}\n",
    "\n",
    "   \n",
    "# Define the base paths\n",
    "bronze_base = \"abfss://bronze@blinkitdl.dfs.core.windows.net/\"\n",
    "silver_base = \"abfss://silver@blinkitdl.dfs.core.windows.net/\"\n",
    "meta_base = \"blinkit.silver.\"\n",
    "\n",
    "# Loop through each table and apply transformations\n",
    "for table_name, transformation_fn in transformations.items():\n",
    "    print(f\"Processing: {table_name}\")\n",
    "    \n",
    "    bronze_path = f\"{bronze_base}{table_name}\"\n",
    "    silver_path = f\"{silver_base}{table_name}\"\n",
    "    meta_silver = f\"{meta_base}{table_name}\"\n",
    "    # Load the data from the Bronze layer\n",
    "    \n",
    " \n",
    "    df = spark.read.format(\"parquet\").option(\"header\",True).option(\"inferSchema\",True).load(bronze_path)\n",
    "    \n",
    "    # Apply each transformation sequentially\n",
    "    transformed_df = transformation_fn(df)\n",
    "    \n",
    "    # Write to the Silver layer\n",
    "    transformed_df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"path\",silver_path) \\\n",
    "        .saveAsTable(meta_silver)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
